{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "bMnxSeGuLkWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PxTqnd0rMqUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87b2c27-ccc6-4059-9ee7-f875fb8f9d21"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load result metrics files from Google Drive"
      ],
      "metadata": {
        "id": "ABX5Ksg-LopS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/cdle'\n",
        "csv_files = glob.glob(folder_path + '/*.csv')\n",
        "\n",
        "for file in csv_files:\n",
        "    print(os.path.basename(file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxADZ0V4KhB0",
        "outputId": "f0d6b754-d622-4aa8-f1c6-49c74ec97b97"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local_benchmark_20250510_140935.csv\n",
            "local_benchmark_20250511_124246.csv\n",
            "local_benchmark_20250511_124435.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "_BjoRYOhRgU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate(ax):\n",
        "  \"\"\"Annotate the height of each bar in the plot.\"\"\"\n",
        "  for p in ax.patches:\n",
        "    ax.annotate(\"%.2fs\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "\n",
        "\n",
        "def annotate_x_times_faster(ax, x_times_list):\n",
        "  \"\"\"Annotate how many times faster is a processing tool per operation in the plot.\"\"\"\n",
        "  num_ops = len(x_times_list)\n",
        "  for i, p in enumerate(ax.patches):\n",
        "    if i < num_ops:\n",
        "      ax.annotate(\"%.1fx\" % x_times_list[i], (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(4, 10), textcoords='offset points', fontsize=8, weight='bold', color=\"#585858\")\n",
        "\n",
        "\n",
        "\n",
        "def create_dual_comparison_plots(plot_title, df, first_tool, second_tool):\n",
        "  \"\"\"Create the plots for comparison of metrics between processing tools.\"\"\"\n",
        "  # Plot with increased size\n",
        "  ax = df.sort_index().plot.bar(\n",
        "      title=plot_title,\n",
        "      figsize=(20, 8)  # ← Change this as needed\n",
        "  )\n",
        "  ax.set_ylabel(\"Elapsed time (sec)\")\n",
        "\n",
        "  # Compute ratios\n",
        "  tmp_df_x_times_faster = df.sort_index().copy()\n",
        "  tmp_df_x_times_faster[f'{first_tool} / {second_tool}'] = tmp_df_x_times_faster.dask / tmp_df_x_times_faster.koalas\n",
        "\n",
        "  # Annotate speedup\n",
        "  annotate_x_times_faster(ax, x_times_list=tmp_df_x_times_faster[f'{first_tool} / {second_tool}'].to_list())\n",
        "\n",
        "  # Plot log-scaled version with same increased size\n",
        "  df.sort_index().plot.bar(\n",
        "      logy=True,\n",
        "      title=f'{plot_title} - log scaling',\n",
        "      figsize=(20, 8)\n",
        "  ).set_ylabel(\"Elapsed time (sec)\")\n",
        "\n",
        "\n",
        "\n",
        "def create_full_comparison_plots(plot_title, df):\n",
        "  \"\"\"Create the plots for comparison of metrics between all processing tools.\"\"\"\n",
        "  # Plot with increased size\n",
        "  ax = df.sort_index().plot.bar(\n",
        "      title=plot_title,\n",
        "      figsize=(20, 8)  # ← Change this as needed\n",
        "  )\n",
        "  ax.set_ylabel(\"Elapsed time (sec)\")\n",
        "\n",
        "  # Plot log-scaled version with same increased size\n",
        "  df.sort_index().plot.bar(\n",
        "      logy=True,\n",
        "      title=f'{plot_title} - log scaling',\n",
        "      figsize=(20, 8)\n",
        "  ).set_ylabel(\"Elapsed time (sec)\")\n",
        "\n",
        "\n",
        "\n",
        "def fair_avg(durations):\n",
        "  \"\"\"Get an average duration among multiple durations fairly by removing the first run and the best run first.\"\"\"\n",
        "  durations = durations[1:]\n",
        "  durations.remove(min(durations))\n",
        "  return sum(durations) / len(durations)\n",
        "\n",
        "\n",
        "\n",
        "def rename_index(df):\n",
        "    \"\"\"Set 'task' as index and rename operations in the index for clarity.\"\"\"\n",
        "    df = df.set_index('task')\n",
        "    df.index = pd.Index([\n",
        "        s.replace(\"filtered \", \"\")\n",
        "         .replace(\"cache \", \"\")\n",
        "         .replace(\"local \", \"\")\n",
        "         .replace(\"of columns\", \"of series\")\n",
        "         .replace(\"addition of series\", \"series addition\")\n",
        "         .replace(\"multiplication of series\", \"series multiplication\")\n",
        "         .replace(\"arithmetic ops\", \"arithmetic\")\n",
        "         .replace(\"count index length\", \"count index\")\n",
        "        for s in df.index\n",
        "    ])\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def avg_result_df(file_name_prefix):\n",
        "  \"\"\"Get result files with the given prefix and then construct the average result dataframe.\"\"\"\n",
        "  dfs = []\n",
        "\n",
        "  # Loop over the actual list of CSV file paths\n",
        "  for file_path in csv_files:\n",
        "      filename = os.path.basename(file_path)\n",
        "      if filename.startswith(file_name_prefix):\n",
        "          dfs.append(pd.read_csv(file_path))\n",
        "\n",
        "  print(f'{file_name_prefix} has {len(dfs)} runs')\n",
        "\n",
        "  # Now compute the average\n",
        "  avg_df = dfs[0].copy()\n",
        "\n",
        "  for op in dfs[0].index:\n",
        "      for lib in ['koalas', 'dask', 'modin', 'joblib', 'spark']:\n",
        "          durations = [df.loc[op][lib] for df in dfs]\n",
        "          avg_df.loc[op, lib] = fair_avg(durations)\n",
        "\n",
        "  print(f\"Loaded {len(dfs)} CSV files into DataFrames.\")\n",
        "\n",
        "  for i, file_path in enumerate(csv_files):\n",
        "    print(f\"{i+1}. {os.path.basename(file_path)}\")\n",
        "\n",
        "  return avg_df"
      ],
      "metadata": {
        "id": "d9JU3fcD6Qio"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the results dataframe with the average values"
      ],
      "metadata": {
        "id": "laiwxQG4RqkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_mode = 'local' # 'local' or 'distributed'\n",
        "benchmark_df = rename_index(avg_result_df(benchmark_mode))\n",
        "benchmark_df.head(10)"
      ],
      "metadata": {
        "id": "FMS4DojdUafr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate dataframes\n",
        "\n",
        "Separate the metrics dataframe in 3 new dataframes: Standard operations, Standard operations with filter and Standard operations with filter and cache"
      ],
      "metadata": {
        "id": "7stZpd5uR2vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_ops = benchmark_df.iloc[:15]\n",
        "ops_with_filtering = benchmark_df.iloc[15:30]\n",
        "ops_with_filtering_and_cache = benchmark_df.iloc[30:]\n",
        "\n",
        "standard_ops.head(15)"
      ],
      "metadata": {
        "id": "O96qfbaEOcvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create insights for comparison between 2 tools\n",
        "\n"
      ],
      "metadata": {
        "id": "xSKqZ8D-MBiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['dask', 'koalas'] # 'dask', 'koalas', 'modin', 'joblib', 'spark'\n",
        "df = standard_ops # 'standard_ops', 'ops_with_filtering', 'ops_with_filtering_and_cache'\n",
        "plot_title=f'Operations with filtering ({benchmark_mode})' # 'Operations with filtering',  'Operations with filtering and cache', 'Standard Operations'\n",
        "\n",
        "create_dual_comparison_plots(plot_title, standard_ops[selected_columns], selected_columns[0], selected_columns[1])"
      ],
      "metadata": {
        "id": "MZx_0aeX6cU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show metrics"
      ],
      "metadata": {
        "id": "Gasm6P9uXHos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotate(pd.Series(stats.gmean(df[selected_columns]), index=[selected_columns[0], selected_columns[1]]).plot.bar(title='Geometric mean'))"
      ],
      "metadata": {
        "id": "rFK9hH2QLzLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotate(df[selected_columns].sum().plot.bar(title='Total execution time'))"
      ],
      "metadata": {
        "id": "kRZPRzz5L2fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_df_x_times_faster = df.sort_index().copy()\n",
        "tmp_df_x_times_faster[f'[{selected_columns[0]} / {selected_columns[1]}'] = tmp_df_x_times_faster[selected_columns[0]] / tmp_df_x_times_faster[selected_columns[1]]\n",
        "tmp_df_x_times_faster"
      ],
      "metadata": {
        "id": "kcXcZ6b8Lu72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create insights for comparison between all tools\n"
      ],
      "metadata": {
        "id": "Cg-kh87pW5NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = standard_ops # 'standard_ops', 'ops_with_filtering', 'ops_with_filtering_and_cache'\n",
        "plot_title=f'Operations with filtering ({benchmark_mode})' # 'Operations with filtering',  'Operations with filtering and cache', 'Standard Operations'\n",
        "\n",
        "create_full_comparison_plots(plot_title, df)"
      ],
      "metadata": {
        "id": "_WiRABdnWlDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance diff %% (simple avg): %s\" % (sum(df[selected_columns[0]] / df[selected_columns[1]]) / len(df)))\n",
        "print(\"Performance diff %% (geomean): %s\" % stats.gmean(df[selected_columns[0]] / df[selected_columns[1]]))\n",
        "\n",
        "\n",
        "arithmetic_ops = df.filter(items=['complex arithmetic', 'series multiplication', 'series addition'], axis=0)\n",
        "print(\"Performance diff (arthemetic) %% (simple avg): %s\" % (sum(arithmetic_ops[selected_columns[0]] / arithmetic_ops[selected_columns[1]]) / len(arithmetic_ops)))\n",
        "print(\"Performance diff (arthemetic) %% (geomean): %s\" % stats.gmean(arithmetic_ops[selected_columns[0]] / arithmetic_ops[selected_columns[1]]))\n",
        "\n",
        "basic_stats_ops = df.filter(items=['count', 'mean', 'standard deviation', 'count index', 'join', 'join count'], axis=0)\n",
        "print(\"Performance diff (basic stats) %% (simple avg): %s\" % (sum(basic_stats_ops[selected_columns[0]] / basic_stats_ops[selected_columns[1]]) / len(basic_stats_ops)))\n",
        "print(\"Performance diff (basic stats) %% (geomean): %s\" % stats.gmean(basic_stats_ops[selected_columns[0]] / basic_stats_ops[selected_columns[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIERmLbSL4-i",
        "outputId": "619511da-0944-4aee-b671-d6ca866e45c0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance diff % (simple avg): 2.529296137793361\n",
            "Performance diff % (geomean): 0.5960303014107625\n",
            "Performance diff (arthemetic) % (simple avg): 8.694845825478447\n",
            "Performance diff (arthemetic) % (geomean): 7.584173893794975\n",
            "Performance diff (basic stats) % (simple avg): 1.0945918566471413\n",
            "Performance diff (basic stats) % (geomean): 0.20501112863815132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall\n"
      ],
      "metadata": {
        "id": "fWiIF0K4Mjbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_df = pd.concat([standard_ops, ops_with_filtering, ops_with_filtering_caching])\n",
        "print(\"Total performance diff %% (simple avg): %s\" % (sum(overall_df[selected_columns[0]] / overall_df[selected_columns[1]]) / len(overall_df)))\n",
        "print(\"Total performance diff %% (geomean): %s\" % stats.gmean(overall_df[selected_columns[0]] / overall_df[selected_columns[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rRlSUeFTMkS5",
        "outputId": "9b36b6e0-5b30-48f4-f7fa-dd0b7e5261fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'standard_ops' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9e4120cf92f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstandard_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops_with_filtering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops_with_filtering_caching\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total performance diff %% (simple avg): %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Koalas (PySpark)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total performance diff %% (geomean): %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Koalas (PySpark)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'standard_ops' is not defined"
          ]
        }
      ]
    }
  ]
}