{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6359c4a-3aa2-4633-913f-857eb65693ef",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "<div style=\"text-align: justify\">\n",
    "In this phase, we colect various metrics including precision, F1 score, recall, accuracy, ROC AUC OVR and ROC AUC for KNN models. We employ cross_validate() to explore combinations of hyperparameters. The goal is to assess a CSV containing metrics for KNN models and hyperparameter subsets, discerning their performance variations across tasks sourced from the OpenML repository.\n",
    "\n",
    "To streamline the process, we partition the OpenML-CC18 Curated classification dataset into segments, distinguishing between multiclass datasets, balanced binary datasets, and imbalanced binary datasets. We employ a threshold criterion, set at 0.3, to determine whether a binary dataset is balanced or imbalanced. Specifically, if one class constitutes less than or equal to 30% of the total targets, the dataset is classified as imbalanced.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6ceb0c-26b7-4f73-a52d-c39adae49e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import openml\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa4dcf-13e9-4e45-afce-2f4c65b0e915",
   "metadata": {},
   "source": [
    "The function separate_dataset_characteristics() is responsible to split the datasets of OpenML-CC18 into\n",
    "- Disbalanced binary tasks\n",
    "- Balanced binary tasks\n",
    "- Multiclasstasks\n",
    "  \n",
    "Futhermore, in this function, we can filter datasets by it number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f43520-d026-4a7a-bdbf-02855144684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset_characteristics(benchmark: str =\"OpenML-CC18\", disbalance_threshold: float = 0.3) -> dict:\n",
    "    benchmark_suite = openml.study.get_suite(benchmark)\n",
    "    subset_benchmark_suite = benchmark_suite.tasks[0:50]\n",
    "    disbalanced_binary_tasks = []\n",
    "    balanced_binary_tasks = []\n",
    "    multiclass_tasks = []\n",
    "\n",
    "    for task_id in subset_benchmark_suite:\n",
    "        task = openml.tasks.get_task(task_id)\n",
    "        _, targets = task.get_X_and_y()\n",
    "        num_classes = len(np.unique(targets))\n",
    "        \n",
    "        num_instances = len(targets)\n",
    "        if num_instances > 15000:\n",
    "            print(f\"Dataset {task_id} too big. Discarted\")\n",
    "            continue\n",
    "\n",
    "        if num_classes == 2:  \n",
    "            minority_fraction = pd.Series(targets).value_counts(normalize=True).min()\n",
    "            if minority_fraction < disbalance_threshold:  disbalanced_binary_tasks.append(task_id)\n",
    "            else: balanced_binary_tasks.append(task_id)\n",
    "            continue\n",
    "\n",
    "        multiclass_tasks.append(task_id)\n",
    "\n",
    "    return {\n",
    "        \"disbalanced_binary_tasks\": disbalanced_binary_tasks,\n",
    "        \"balanced_binary_tasks\": balanced_binary_tasks,\n",
    "        \"multiclass_tasks\": multiclass_tasks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427b58c-3e97-4ec5-b348-4839aa212026",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The function run_benchmark() trains and evaluates all subsets of models using different hyperparameters. It then incorporates metrics, the number of tasks, dataset type (extracted separately using separate_dataset_characteristics()), and the analyzed model into a dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85dedd5d-a500-4c4d-ab59-657a92f0febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model: any, model_name: str, params=None, metrics: dict = None, tasks: list = None, tasks_description: str = None) -> DataFrame:\n",
    "    print(f\"\\nEvaluating metrics for {model_name} model\")\n",
    "    results = pd.DataFrame(columns=[\"dataset\", \"model\", \"neighbours\", \"weights\", \"tasks_description\"]) \n",
    "\n",
    "    if tasks is None or tasks == []:\n",
    "        return \n",
    "\n",
    "    for task_id in tasks:\n",
    "        print(f\"Started task {task_id}\")\n",
    "        task = openml.tasks.get_task(task_id)\n",
    "        features, targets = task.get_X_and_y()\n",
    "        \n",
    "        cv_results = cross_validate(model, features, targets, cv=10, scoring=metrics) \n",
    "        scores = {metric: np.mean(cv_results[f'test_{metric}']) if f'test_{metric}' in cv_results.keys() else np.nan for metric in metrics}\n",
    "        scores.update({\"dataset\": task_id, \"model\": model_name, \"neighbours\": params['kneighborsclassifier__n_neighbors'], \"weights\": params['kneighborsclassifier__weights'] ,\"tasks_description\": tasks_description})\n",
    "        \n",
    "        results = pd.concat([results, pd.DataFrame(scores, index=[0])], ignore_index=True) \n",
    "\n",
    "    print(\"Finalized evaluation\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f288e590-7076-4715-9f5f-4396ccf07e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concat_list_of_dataframes(list_of_dataframes: list) -> DataFrame:\n",
    "    if list_of_dataframes: return pd.concat(list_of_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "def create_csv(dataframe: DataFrame, name: str) -> None:\n",
    "    path = '../files_csv'\n",
    "    file_path = os.path.join(path, name)\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "    print(f\"CSV file '{name}' saved successfully in '{file_path}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646dbbac-e85a-4c79-aa0c-a2652fc00b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(model: any, model_name: str, params: dict, metrics: list, tasks: dict) -> None:\n",
    "    try:\n",
    "        print(f\"\\nModel Name: {model_name}\")\n",
    "        print(f\"Keys in params: {params.keys()}\")\n",
    "        \n",
    "        all_results = []  \n",
    "\n",
    "        for param_combination in ParameterGrid(params):\n",
    "            model_instance = model.set_params(**param_combination)\n",
    "            print(f\"Running benchmark for parameters: {param_combination}\")\n",
    "            \n",
    "            for task_type, task_list in tasks.items():\n",
    "                for task in task_list:\n",
    "                    task_result = run_benchmark(model_instance, model_name, params=param_combination, metrics=metrics, tasks=[task], tasks_description=task_type)\n",
    "                    all_results.append(task_result)\n",
    "            \n",
    "\n",
    "        # Concatenate all results\n",
    "        concatenated_df = pd.concat(all_results)\n",
    "\n",
    "        if not concatenated_df.empty:\n",
    "            create_csv(concatenated_df, f\"metrics_{model_name}.csv\")\n",
    "            print(f\"Created csv metrics_{model_name}.csv\")\n",
    "        else:\n",
    "            print(f\"No dataframes to concatenate for {model_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df35b5-71c2-4071-8b2f-24b23d5dfe94",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Here, we instantiate the models to run the benchmark, then select the metrics for evaluation and the hyperparameters for each KNN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283e91c-c59f-4242-9af6-9b45ad101cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = separate_dataset_characteristics()\n",
    "print(\"\\nDisbalanced binary tasks: \", tasks['disbalanced_binary_tasks'])\n",
    "print(\"Balanced binary tasks: \", tasks['balanced_binary_tasks'])\n",
    "print(\"Multiclass tasks: \", tasks['multiclass_tasks'])\n",
    "\n",
    "models_to_run = {\n",
    "    'knn': {\n",
    "        'model': make_pipeline(SimpleImputer(strategy='constant'), StandardScaler(), KNeighborsClassifier()),\n",
    "        'params': {\n",
    "            'kneighborsclassifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15],  \n",
    "            'kneighborsclassifier__weights': ['uniform', 'distance'],  \n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'roc_auc_ovr']\n",
    "\n",
    "for model_name, model_info in models_to_run.items():\n",
    "    extract_metrics(model_info['model'], model_name, model_info['params'], metrics, tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b463fec-2f93-4c43-a5da-59b2d616e53a",
   "metadata": {},
   "source": [
    "After running all the code, willAfter executing the code, a CSV file containing selected metrics and hyperparameters of each model will be generated. With this CSV, we can analyze the data to identify areas where the KNN model may be underperforming and take appropriate actions.\n",
    " be generated a csv file with all the chosen metrics and huperparameters of each model. With this csv, we can analyse the data in it and see where the knn is being not so good and meke something about it:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
